---
title: Questions
parent: AI
nav_order: 2
---
## Do LLMs actually know anything?

For example, can an LLM *know* if dogs are hairy without knowing what dogs are or what hairiness means?

Altman and Sutskever are amusingly coy on the subject.

"Well, they certainly learnt something" - [Sam Altman](https://www.youtube.com/watch?v=L_Guz73e6fw)

"Saying what understanding means is hard, and so we measure prediction instead" - [Ilya Sutskever](https://www.youtube.com/watch?v=Wmo2vR7U9ck)

This is part of "intelligence equals prediction", which maybe explains how OpenAI thought of all this in the first place.

## Representing ideas

What did [Sutskever](https://www.youtube.com/watch?v=SjhIlw3Iffs) mean when he said "an LLM is great for learning about the world (including representations of ideas), but not so good for producing output"?

Here is [Gemini's](https://gemini.google.com/share/50b37f74dd86) answer.

## Transformer Multitasking

Here is another Sutskever quote:

> during training the transformer is forced to multitask a huge number of tasks

And here are [Gemini's](https://gemini.google.com/share/470a057491fe) thoughts on what he meant.
